// File generated by hadoop record compiler. Do not edit.
package juterc;

import juterc.RcUtil;

public class MyDataType extends org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable {
  private static final org.apache.hadoop.record.meta.RecordTypeInfo _rio_recTypeInfo;
  private static org.apache.hadoop.record.meta.RecordTypeInfo _rio_rtiFilter;
  private static int[] _rio_rtiFilterFields;
  static {
    _rio_recTypeInfo = new org.apache.hadoop.record.meta.RecordTypeInfo("MyDataType");
    _rio_recTypeInfo.addField("name", org.apache.hadoop.record.meta.TypeID.StringTypeID);
    _rio_recTypeInfo.addField("genda", org.apache.hadoop.record.meta.TypeID.StringTypeID);
  }
  
  private String name;
  private String genda;
  public MyDataType() { }
  public MyDataType(
    final String name,
    final String genda) {
    this.name = name;
    this.genda = genda;
    this.serialize();
  }
  public static org.apache.hadoop.record.meta.RecordTypeInfo getTypeInfo() {
    return _rio_recTypeInfo;
  }
  public static void setTypeFilter(org.apache.hadoop.record.meta.RecordTypeInfo rti) {
    if (null == rti) return;
    _rio_rtiFilter = rti;
    _rio_rtiFilterFields = null;
  }
  private static void setupRtiFields()
  {
    if (null == _rio_rtiFilter) return;
    // we may already have done this
    if (null != _rio_rtiFilterFields) return;
    int _rio_i, _rio_j;
    _rio_rtiFilterFields = new int [_rio_rtiFilter.getFieldTypeInfos().size()];
    for (_rio_i=0; _rio_i<_rio_rtiFilterFields.length; _rio_i++) {
      _rio_rtiFilterFields[_rio_i] = 0;
    }
    java.util.Iterator<org.apache.hadoop.record.meta.FieldTypeInfo> _rio_itFilter = _rio_rtiFilter.getFieldTypeInfos().iterator();
    _rio_i=0;
    while (_rio_itFilter.hasNext()) {
      org.apache.hadoop.record.meta.FieldTypeInfo _rio_tInfoFilter = _rio_itFilter.next();
      java.util.Iterator<org.apache.hadoop.record.meta.FieldTypeInfo> _rio_it = _rio_recTypeInfo.getFieldTypeInfos().iterator();
      _rio_j=1;
      while (_rio_it.hasNext()) {
        org.apache.hadoop.record.meta.FieldTypeInfo _rio_tInfo = _rio_it.next();
        if (_rio_tInfo.equals(_rio_tInfoFilter)) {
          _rio_rtiFilterFields[_rio_i] = _rio_j;
          break;
        }
        _rio_j++;
      }
      _rio_i++;
    }
  }
  public String getName() {
    return name;
  }
  public void setName(final String name) {
    this.name=name;
  }
  public String getGenda() {
    return genda;
  }
  public void setGenda(final String genda) {
    this.genda=genda;
  }
  public void serialize() {
    int writeIndx = 0;
    try {
      juterc.RcUtil.writeString(this, name, writeIndx++);
      juterc.RcUtil.writeString(this, genda, writeIndx++);
    } catch(java.io.IOException e) {
      e.printStackTrace();
    }
  }
  public void deserialize(org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable bra){
    int readIndx = 0;
    try {
      name=juterc.RcUtil.readString(bra, readIndx++);
      genda=juterc.RcUtil.readString(bra, readIndx++);
    } catch(java.io.IOException e) {
      e.printStackTrace();
    }
  }
}
